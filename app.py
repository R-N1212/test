# RAG_vectorãƒ•ã‚¡ã‚¤ãƒ«äºŒã¤ã‚’å¯¾è±¡ã¨ã™ã‚‹å½¢å¼ï¼ˆãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã€FAQï¼‰

import streamlit as st
import pickle
import numpy as np
import os
from sentence_transformers import SentenceTransformer
from openai import OpenAI

# === âœ… ãƒšãƒ¼ã‚¸è¨­å®š ===
st.set_page_config(page_title="ç¤¾å†…RAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ", layout="wide")
st.title("ğŸ“˜ ç¤¾å†…ãƒ«ãƒ¼ãƒ«ç›¸è«‡ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")

# === âœ… ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ï¼ˆFAQ + ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ï¼‰ ===
@st.cache_resource
def load_model_and_data():
    model = SentenceTransformer("all-MiniLM-L6-v2")

    # FAQãƒ‡ãƒ¼ã‚¿
    with open("vector_store_faq.pkl", "rb") as f:
        faq_data = pickle.load(f)

    # ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ãƒ‡ãƒ¼ã‚¿
    with open("vector_store.pkl", "rb") as f:
        manual_data = pickle.load(f)

    return model, faq_data, manual_data

model, faq_data, manual_data = load_model_and_data()

# === âœ… OpenAIã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿ï¼ˆsecrets.toml çµŒç”±ï¼‰ ===
api_key = st.secrets["OPENAI_API_KEY"]
client = OpenAI(api_key=api_key)

# === âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›æ¬„ ===
query = st.text_input("ğŸ’¬ è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

if st.button("å›ç­”ã‚’ç”Ÿæˆ") and query:
    query_vec = model.encode(query)

    def search_similar_docs(data, query_vec, top_k=3):
        embeddings = [model.encode(d['body']) for d in data]
        similarities = np.dot(embeddings, query_vec) / (
            np.linalg.norm(embeddings, axis=1) * np.linalg.norm(query_vec)
        )
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        return [data[i] for i in top_indices]

    # === âœ… é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå–å¾—ï¼ˆFAQ + ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ï¼‰ ===
    top_faq = search_similar_docs(faq_data, query_vec)
    top_manual = search_similar_docs(manual_data, query_vec)

    # === âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ ===
    def format_docs(label, docs):
        return f"\n--- {label} ---\n" + "\n".join(
            f"{doc.get('title', 'Q&A')}ï¼ˆ{doc.get('article', '')}ï¼‰: {doc['body']}"
            for doc in docs
        )

    retrieved_text = format_docs("FAQ", top_faq) + "\n\n" + format_docs("ãƒãƒ‹ãƒ¥ã‚¢ãƒ«", top_manual)

    prompt = f"""ä»¥ä¸‹ã®ç¤¾å†…è³‡æ–™ã«åŸºã¥ãã€è³ªå•ã«ä¸å¯§ã«ç­”ãˆã¦ãã ã•ã„ã€‚

{retrieved_text}

è³ªå•:
{query}

å›ç­”:"""

    # === âœ… ChatGPTã§å›ç­”ç”Ÿæˆ ===
    with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­..."):
        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯ç¤¾å†…è¦å®šã«è©³ã—ã„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                    {"role": "user", "content": prompt},
                ],
                temperature=0.3,
            )
            answer = response.choices[0].message.content
            st.success("âœ… å›ç­”ã¯ã“ã¡ã‚‰")
            st.markdown(answer)
        except Exception as e:
            st.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

elif not query:
    st.info("ğŸ’¬ è³ªå•ã‚’å…¥åŠ›å¾Œã€[å›ç­”ã‚’ç”Ÿæˆ]ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„")
